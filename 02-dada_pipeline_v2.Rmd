---
title: "DADA2 pipeline"
author: "Stephen Noell"
date: "21/06/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE, 
                      warning = FALSE,
                      fig.align = "center",
                      fig.width = 10,
                      fig.height = 6)
```


```{r}

# Libraries
library("dada2")
library(ShortRead)
library(Biostrings)
library(seqinr)
library(DECIPHER)
library(readr)
library(phangorn)
library("tidyverse")       # data wrangling and visualisation
library("phyloseq")        # analysis of microbial communities
library("here")            # set the path to the folder
library("ggplot2")
library("ggpubr") #for putting figs together
library("RColorBrewer") #for color palettes
library("svglite") #for saving SVGs

set.seed(57)
```


```{r}
# Read the names in the fastaq files 
fn_f <- sort(list.files(path = "C:/Users/snoell/OneDrive - The University of Waikato/Documents/MGS00691_John_Longmore_Delivery/MGS00691_1/final.cutadapt" , pattern="_R1_cutadapt.fastq.gz", full.names = TRUE))

fn_r <- sort(list.files(path = "C:/Users/snoell/OneDrive - The University of Waikato/Documents/MGS00691_John_Longmore_Delivery/MGS00691_1/final.cutadapt" , pattern="_R2_cutadapt.fastq.gz", full.names = TRUE))

# Give the sample names
sample.names <- c("Legin_13.02_30", "Legin_13.02_90", "Legin_13.02_120", "Legin_13.02_130", "Legin_13.02_140", "Legin_22.02_30", "Legin_22.02_90", "Legin_22.02_120", "Legin_22.02_130", "Legin_22.02_140", "Legin_22.02_150", "Legin_11.03_50", "Legin_11.03_100", "Legin_11.03_120", "Legin_11.03_130", "Legin_11.03_140", "Legin_11.03_150", "Legin_03.04_130", "Legin_03.04_140", "Legin_03.04_150", "Legin_03.04_160", "Legin_03.04_170", "Legin_03.04_180")

read.names_f <- c("Legin_13.02_30_R1", "Legin_13.02_90_R1", "Legin_13.02_120_R1", "Legin_13.02_130_R1", "Legin_13.02_140_R1", "Legin_22.02_30_R1", "Legin_22.02_90_R1", "Legin_22.02_120_R1", "Legin_22.02_130_R1", "Legin_22.02_140_R1", "Legin_22.02_150_R1", "Legin_11.03_50_R1", "Legin_11.03_100_R1", "Legin_11.03_120_R1", "Legin_11.03_130_R1", "Legin_11.03_140_R1", "Legin_11.03_150_R1", "Legin_03.04_130_R1", "Legin_03.04_140_R1", "Legin_03.04_150_R1", "Legin_03.04_160_R1", "Legin_03.04_170_R1", "Legin_03.04_180_R1")

read.names_r <- c("Legin_13.02_30_R2", "Legin_13.02_90_R2", "Legin_13.02_120_R2", "Legin_13.02_130_R2", "Legin_13.02_140_R2", "Legin_22.02_30_R2", "Legin_22.02_90_R2", "Legin_22.02_120_R2", "Legin_22.02_130_R2", "Legin_22.02_140_R2", "Legin_22.02_150_R2", "Legin_11.03_50_R2", "Legin_11.03_100_R2", "Legin_11.03_120_R2", "Legin_11.03_130_R2", "Legin_11.03_140_R2", "Legin_11.03_150_R2", "Legin_03.04_130_R2", "Legin_03.04_140_R2", "Legin_03.04_150_R2", "Legin_03.04_160_R2", "Legin_03.04_170_R2", "Legin_03.04_180_R2")

```

```{r}
# Check quality score of reads ----
plotQualityProfile(fn_f[1])

# Check for primers ----
# Set up the primer sequences 
# where to check primer sequences - https://www.earthmicrobiome.org/protocols-and-standards/16s/

FWD <- "GTGYCAGCMGCCGCGGTAA"   ## 515F (Parada) 
REV <- "CCGYCAATTYMTTTRAGTTT"  ## 926R (Quince) 


# Write a function that creates a list of all orientations of the primers
allOrients <- function(primer) {
  dna <- DNAString(primer)  
  orients <- c(Forward = dna, Complement = Biostrings::complement(dna), Reverse = reverse(dna), 
               RevComp = reverseComplement(dna))
  return(sapply(orients, toString))  # Convert back to character vector
}

# Primer orientations - forward / complement / reverse / reverse complement
FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)


# Write a function that counts how many time primers appear in a sequence
primerHits <- function(primer, fns) {
  nhits <- vcountPattern(primer, sread(readFastq(fns)), fixed = FALSE)
  return(sum(nhits > 0))
}

# See if sequences have primer - in this example sample #1, change to check others
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fns = fn_f[[4]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fns = fn_r[[4]]))


# How can I remove primers/adapters/etc from my amplicon reads? - check FAQ
# https://benjjneb.github.io/dada2/faq.html

# If primers are at the start of your reads and are a constant length (it's our case) 
# you can use the trimLeft argument of dada2s filtering functions to remove the primers

path <- "C:/Users/snoell/OneDrive - The University of Waikato/Documents/MGS00691_John_Longmore_Delivery/MGS00691_1"

# Create a directory for the filtered files
filt_f <- file.path(path, "final.cutadapt.dada2", paste0(read.names_f, "_filt.fastq.gz"))
filt_r <- file.path(path, "final.cutadapt.dada2", paste0(read.names_r, "_filt.fastq.gz"))

out <- filterAndTrim(fn_f, filt_f, rev = fn_r, filt.rev = filt_r, truncLen = 265, maxN=0, maxEE=2, truncQ=2, 
                      compress=TRUE, multithread=FALSE)
out

path2 <- "C:/Users/snoell/OneDrive - The University of Waikato/Documents/MGS00691_John_Longmore_Delivery/MGS00691_1/final.cutadapt.dada2"

# Read the names in the cutadapt trimmed fastq.gz files
fn.filt <- sort(list.files(path2, pattern=".fastq.gz", full.names = TRUE))

# Look at primer detection for the a number of samples
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fns = fn.filt[[20]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fns = fn.filt[[20]]))


plotQualityProfile(fn.filt[1:6])

```

```{r}
# Learn the errors ----

# Learn the error Rates
err_f <- learnErrors(filt_f, multithread=TRUE)
err_r <- learnErrors(filt_r, multithread=TRUE)

# visualize the estimated error rates:
plotErrors(err_f, nominalQ=TRUE)

```


```{r}
# Sample Inference ----

# dada command to get the seqeunces
dadaf <- dada(filt_f, err_f, multithread=TRUE)
dadar <- dada(filt_r, err_r, multithread=TRUE)

mergers <- mergePairs(dadaf, filt_f, dadar, filt_r, verbose=TRUE)

# Construct sequence table
seqtab <- makeSequenceTable(mergers)

dim(seqtab)

#23 5352

# The sequence table is a matrix with rows corresponding to (and named by) the samples
# and columns corresponding to (and named by) the sequence variants

# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))

#remove some too small or too big ones
seqtab2 <- seqtab[,nchar(colnames(seqtab)) %in% 368:400]
table(nchar(getSequences(seqtab2)))

```

```{r}

# Remove chimeras - Chimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant parent sequences
seqtab.nochim <- removeBimeraDenovo(seqtab2, method="consensus", multithread=TRUE, verbose=TRUE)
# Identified 835 bimeras out of 1853 input sequences

dim(seqtab.nochim)

sum(seqtab.nochim)/sum(seqtab2)
# [1] 0.81 - the frequency of non-chimeric sequences

```

```{r}

# Track reads through the pipeline - look at the number of reads that made it through each step in the pipeline

getN <- function(x) sum(getUniques(x))

track <- cbind(out, sapply(dadaf, getN), rowSums(seqtab.nochim))
colnames(track) <- c("reads.in", "filter.and.trim", "sample.inference", "nonchim")
rownames(track) <- sample.names
track2 <- as.data.frame(track) %>%
  dplyr::mutate(prop = nonchim / reads.in)

track2

# outside of filtering there should be no step in which a majority of reads are lost
# have at least thousands of sequences per-sample, to have good resolution down to 1% frequency - https://github.com/benjjneb/dada2/issues/232
# keep a good proportion of the reads (i.e. >>10%) all the way through the pipeline

# Save the ASV table
#saveRDS(seqtab.nochim, "seqtab.nochim.es")

# Save as csv
#write.csv(seqtab.nochim, "asv_es.csv")

# Save tracking table
#write.csv(track2, "Table S2_track.csv")

```

```{r}
# Create a fasta table
name.seq <- paste0("ASV", seq(ncol(seqtab.nochim)))
uniqueSeqs <- as.list(colnames(seqtab.nochim))
write.fasta(uniqueSeqs, name.seq, "asvs_no-filt.fasta")
```



```{R}
#ASV table read in
seqtab.nochim <- read.csv("seqtab.nochim_es2.csv") %>%
  column_to_rownames(var = "X") %>%
  as.matrix(.)
  
# Assign taxonomy ---- using DADA2 bayesian classifier

# DADA2 provides a native implementation of the naive Bayesian classifier method for this
# Maintains formatted training fastas - https://benjjneb.github.io/dada2/training.html

# Using the naive bayesian classifier ----

# Silva database; using v. 138.2
taxa.silva <- assignTaxonomy(seqtab.nochim, "C:/Users/snoell/OneDrive - The University of Waikato/Documents/silva_nr99_v138.2_toGenus_trainset.fa.gz", multithread=TRUE)

# Save the taxanomy table
saveRDS(taxa.silva, "taxa.silva")

#taxa.silva <- as.data.frame(readRDS("taxa.silva"))

```


```{r}
# Tree ----
# Get the seqs and store the them so they can be the tips of the tree
es_seqs <- getSequences(seqtab.nochim)
names(es_seqs) <- es_seqs
# Align sequences (using DECIPHER)
alignment <- AlignSeqs(DNAStringSet(es_seqs), anchor = NA,verbose = FALSE)

# Build a tree (using phangorn)
phangAlign <- phyDat(as(alignment, "matrix"), type = "DNA")
dm <- dist.ml(phangAlign)
treeNJ <- NJ(dm) 
fit = pml(treeNJ, data=phangAlign)
fitGTR <- update(fit, k=4, inv=0.2)
fitGTR <- optim.pml(fitGTR, model="GTR", optInv=TRUE, optGamma=TRUE,
                    rearrangement = "stochastic", control = pml.control(trace = 0))

#saveRDS(fitGTR, "fitGTR")

```



```{r}
# Handoff to phyloseq ----
# Load all data

fitGTR <- readRDS("fitGTR")
seqtab.nochim <- readRDS("seqtab.nochim.es")
envdata <-  read.csv("sample_data.csv", row.names = 1, header = TRUE)
envdata$Depth2 <- as.factor(envdata$Depth)
taxa.silva <- as.matrix(readRDS("taxa.silva"))


# Change the row names of the ASV table to match the environmental table 
map_df <- as.data.frame(read.csv("sample_name_map.csv"))
rownames(seqtab.nochim) <- map_df$new[match(rownames(seqtab.nochim), map_df$old)]

#Final phyloseq creation
physeq.es0 <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE), 
                    tax_table(taxa.silva),
                    sample_data(envdata),
                    phy_tree(fitGTR$tree))

#remove low abundance taxa
taxa_sums_total <- taxa_sums(physeq.es0)

physeq.es <- prune_taxa(taxa_sums_total > 10, physeq.es0)

ntaxa(physeq.es0)
ntaxa(physeq.es)
#removes ~90 ASVs with low abundance

new.names <- paste0("ASV_", seq(ntaxa(physeq.es))) # define new names ASV_1, ASV_2, ...
seqs <- taxa_names(physeq.es) # store sequences
names(seqs) <- new.names # make map from ASV to full sequence
taxa_names(physeq.es) <- new.names # rename 

#saveRDS(physeq.es, "physeq.es")

#Save FASTA
seqs2 <- DNAStringSet(seqs)
writeXStringSet(seqs2, filepath = "es_illumina_not-trimmed.fasta")
```

```{r}
#modify sample data file
physeq.es<- readRDS("physeq.es")

sample_data(physeq.es) <- sample_data(envdata)
saveRDS(physeq.es, "physeq.es")
```